{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(mnist_trainset, batch_size=len(mnist_trainset))\n",
    "train_data = next(iter(trainLoader))\n",
    "\n",
    "testLoader = torch.utils.data.DataLoader(mnist_testset, batch_size=len(mnist_testset))\n",
    "test_data = next(iter(testLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => 5923\n",
      "1 => 6742\n",
      "2 => 5958\n",
      "3 => 6131\n",
      "4 => 5842\n",
      "5 => 5421\n",
      "6 => 5918\n",
      "7 => 6265\n",
      "8 => 5851\n",
      "9 => 5949\n",
      "\n",
      "0 => 980\n",
      "1 => 1135\n",
      "2 => 1032\n",
      "3 => 1010\n",
      "4 => 982\n",
      "5 => 892\n",
      "6 => 958\n",
      "7 => 1028\n",
      "8 => 974\n",
      "9 => 1009\n"
     ]
    }
   ],
   "source": [
    "def dataSet2Dict(train_data):\n",
    "    x,y= train_data\n",
    "    data_dict={}\n",
    "    for i in range(x.shape[0]):\n",
    "        if y[i].item() not in data_dict.keys():\n",
    "            data_dict[y[i].item()]=[]\n",
    "        data_dict[y[i].item()].append(x[i])\n",
    "    return data_dict\n",
    "\n",
    "train_data_dict = dataSet2Dict(train_data)\n",
    "test_data_dict = dataSet2Dict(test_data)\n",
    "\n",
    "################################################\n",
    "for k in train_data_dict.keys():\n",
    "    print(str(k)+' => '+str(len(train_data_dict[k])))\n",
    "print()\n",
    "for k in test_data_dict.keys():\n",
    "    print(str(k)+' => '+str(len(test_data_dict[k])))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_data_dict[9][2][0], interpolation='nearest', cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(n, data_dict, digits = [0,1,2,3,4,5,6,7,8,9]):\n",
    "    img1s = np.zeros((n, 1, 28, 28))\n",
    "    img2s = np.zeros((n, 1, 28, 28))\n",
    "    target = np.zeros((n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        p = np.random.rand()\n",
    "        d1, d2 = np.random.choice(digits, 2, replace=False)\n",
    "        img1s[i] = data_dict[d1][ int(np.random.rand()*len(data_dict[d1]))]\n",
    "        \n",
    "        if p > 0.5: # diff\n",
    "            img2s[i] = data_dict[d2][ int(np.random.rand()*len(data_dict[d2]))]\n",
    "            target[i] = 0\n",
    "        else:       # same\n",
    "            img2s[i] = data_dict[d1][ int(np.random.rand()*len(data_dict[d1]))]\n",
    "            target[i] = 1     \n",
    "    \n",
    "    return torch.from_numpy(img1s).float(), torch.from_numpy(img2s).float(), torch.from_numpy(target).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataForTripletLoss(n, data_dict, digits = [0,1,2,3,4,5,6,7,8,9]):\n",
    "    imgs = np.zeros((n, 1, 28, 28))\n",
    "    posis = np.zeros((n, 1, 28, 28))\n",
    "    negis = np.zeros((n, 1, 28, 28))\n",
    "    \n",
    "    for i in range(n):\n",
    "        d1, d2 = np.random.choice(digits, 2, replace=False)\n",
    "        imgs[i] = data_dict[d1][ int(np.random.rand()*len(data_dict[d1]))]\n",
    "        posis[i] = data_dict[d1][ int(np.random.rand()*len(data_dict[d1]))] # sames as img\n",
    "        negis[i] = data_dict[d2][ int(np.random.rand()*len(data_dict[d2]))] # diff from img\n",
    "        \n",
    "    \n",
    "    return torch.from_numpy(imgs).float(), torch.from_numpy(posis).float(), torch.from_numpy(negis).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(nn.Module):    # outputs the distance(calculated using dostFunc) between representations of 2 images\n",
    "\n",
    "    def __init__(self, distFunc = F.cosine_similarity):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.distFunc = distFunc\n",
    "\n",
    "    def calcRep(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        dist = self.distFunc(SiameseNet.calcRep(self, x1), SiameseNet.calcRep(self, x2))  # output 1 for similar image\n",
    "        return dist\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,steps = 1000 ,lr = 0.1, batch_sz = 32, digits = [0,1,2,3,4,5,6,7,8,9]):\n",
    "    model.train()\n",
    "    \n",
    "    for i in range(steps):\n",
    "    \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        img1s, img2s, target = getData(batch_sz, train_data_dict, digits)\n",
    "        output = model(img1s, img2s)\n",
    "        \n",
    "        loss = ((output - target)**2).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('step: '+str(i),end='\\t')\n",
    "#             print(loss.item())\n",
    "            test(model, sz = 1000, digits = [0,1,2,3,4,5,6,7,8,9])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, sz = 1000, digits = [0,1,2,3,4,5,6,7,8,9]):\n",
    "    img1s, img2s, target = getData(sz, test_data_dict, digits)\n",
    "    output = model(img1s, img2s)\n",
    "    loss = ((output - target)**2).mean()\n",
    "    correct = ((output.round()).eq(target.view_as(output))).long().sum().item()    # threshold = .5\n",
    "    total = target.view(-1).size(0)\n",
    "    print('mse = '+str(loss.item())[:10]+'\\t accuracy = '+str(correct)+'/'+str(total)+' = '+str(100.0*correct/total)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainUsingTripletLoss(model,steps = 1000, alpha = 1.0 ,lr = 0.01, batch_sz = 32, digits = [0,1,2,3,4,5,6,7,8,9]):\n",
    "    model.train()\n",
    "\n",
    "    for i in range(steps):\n",
    "    \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        anchor, posis, negis = getDataForTripletLoss(batch_sz, train_data_dict, digits)\n",
    "        pos_dists = model(anchor, posis)\n",
    "        neg_dists = model(anchor, negis)\n",
    "        \n",
    "        loss = (F.relu(pos_dists - neg_dists + alpha)).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('step: '+str(i),end='\\t')\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\tmse = 0.47134423\t accuracy = 528/1000 = 52.8\n",
      "step: 100\tmse = 0.24274824\t accuracy = 664/1000 = 66.4\n",
      "mse = 0.17610462\t accuracy = 3778/5000 = 75.56\n",
      "mse = 0.22212973\t accuracy = 3381/5000 = 67.62\n"
     ]
    }
   ],
   "source": [
    "## USING COSINE DISTANCE\n",
    "model = SiameseNet()\n",
    "train(model,steps = 200 ,lr = 0.1, batch_sz = 32, digits = [0,1,5,8,9])\n",
    "\n",
    "test(model, sz = 5000, digits = [0,1,2,3,4,5,6,7,8,9])\n",
    "test(model, sz = 5000, digits = [2,3,4,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myDist(a,b):\n",
    "    d = torch.from_numpy(np.zeros((a.shape[0]))).float()\n",
    "    for i in range(a.shape[0]):\n",
    "        d[i] = F.mse_loss(a[i],b[i])\n",
    "#     d = F.cosine_similarity(a,b)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\t0.9991343021392822\n",
      "step: 100\t0.031219154596328735\n",
      "step: 200\t0.0\n",
      "step: 300\t0.05829644203186035\n",
      "step: 400\t0.04677845165133476\n",
      "step: 500\t0.0\n",
      "step: 600\t0.16933515667915344\n",
      "step: 700\t0.024208180606365204\n",
      "step: 800\t0.0\n",
      "step: 900\t0.0\n",
      "step: 1000\t0.11985106021165848\n",
      "step: 1100\t0.048132918775081635\n",
      "step: 1200\t0.0\n",
      "step: 1300\t0.0\n",
      "step: 1400\t0.1712268739938736\n",
      "step: 1500\t0.0\n",
      "step: 1600\t0.0\n",
      "step: 1700\t0.01924695074558258\n",
      "step: 1800\t0.03762444108724594\n",
      "step: 1900\t0.03136613965034485\n"
     ]
    }
   ],
   "source": [
    "### USING TRIPLET LOSS\n",
    "model = SiameseNet(distFunc = myDist)\n",
    "trainUsingTripletLoss(model,steps = 2000 ,lr = 0.001, batch_sz = 16, digits = [0,1,5,8,9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "x_train, y_train = train_data\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "# print(x_train.shape)\n",
    "knn.fit(x_train, y_train)\n",
    "# score = knn.score(X_test_pca[:,:component], y_test_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USING KNN\n",
    "x_test, y_test = test_data\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "score = knn.score(x_test, y_test)    #percentage match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9705\n",
      "torch.Size([10000, 784])\n"
     ]
    }
   ],
   "source": [
    "print(score)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 ... 4 5 6]\n",
      "[7 2 1 ... 4 5 6]\n",
      "9705\n"
     ]
    }
   ],
   "source": [
    "print(np.array(predict))\n",
    "print(np.array(y_test))\n",
    "print(np.sum(np.array(y_test) == np.array(predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customDist(x, y):\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(x.shape)\n",
    "    x=x.reshape(x.shape[0],1,28,28)\n",
    "    y=y.reshape(y.shape[0],1,28,28)\n",
    "    return model(x, y).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9999999 , 1.0000002 , 0.99999994, 0.9999999 , 1.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train[:3].shape\n",
    "# train_data[0].shape\n",
    "customDist(x_train[:5], x_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784])\n",
      "[0.96379611 0.34981108 0.50881206 0.47779028 0.60948726 0.30018627\n",
      " 0.65632062 0.0605414  0.8798962  0.14034181]\n",
      "[0.96379611 0.34981108 0.50881206 0.47779028 0.60948726 0.30018627\n",
      " 0.65632062 0.0605414  0.8798962  0.14034181]\n",
      "(10,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 10 into shape (10,1,28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-29b1e75451b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mknnS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    256\u001b[0m             self._tree = BallTree(X, self.leaf_size,\n\u001b[1;32m    257\u001b[0m                                   \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                                   **self.effective_metric_params_)\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'kd_tree'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             self._tree = KDTree(X, self.leaf_size,\n",
      "\u001b[0;32msklearn/neighbors/binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors.ball_tree.BinaryTree.__init__ (sklearn/neighbors/ball_tree.c:8906)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/dist_metrics.pyx\u001b[0m in \u001b[0;36msklearn.neighbors.dist_metrics.DistanceMetric.get_metric (sklearn/neighbors/dist_metrics.c:4141)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/dist_metrics.pyx\u001b[0m in \u001b[0;36msklearn.neighbors.dist_metrics.PyFuncDistance.__init__ (sklearn/neighbors/dist_metrics.c:10864)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-88450fdbbb30>\u001b[0m in \u001b[0;36mcustomDist\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 10 into shape (10,1,28,28)"
     ]
    }
   ],
   "source": [
    "### USING THE TRAINED NET FOR DINTANCE IN KNN\n",
    "knnS = KNeighborsClassifier(n_neighbors=3, metric= customDist)\n",
    "\n",
    "x_train, y_train = train_data\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "\n",
    "x_test, y_test = test_data\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "print(x_train.shape)\n",
    "knnS.fit(x_train, y_train)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
