{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(mnist_trainset, batch_size=len(mnist_trainset))\n",
    "train_data = next(iter(trainLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => 5923\n",
      "1 => 6742\n",
      "2 => 5958\n",
      "3 => 6131\n",
      "4 => 5842\n",
      "5 => 5421\n",
      "6 => 5918\n",
      "7 => 6265\n",
      "8 => 5851\n",
      "9 => 5949\n"
     ]
    }
   ],
   "source": [
    "x,y= train_data\n",
    "data_dict={}\n",
    "for i in range(x.shape[0]):\n",
    "    if y[i].item() not in data_dict.keys():\n",
    "        data_dict[y[i].item()]=[]\n",
    "    data_dict[y[i].item()].append(x[i])\n",
    "\n",
    "for k in data_dict.keys():\n",
    "    print(str(k)+' => '+str(len(data_dict[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data_dict[9][2][0], interpolation='nearest', cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(n, digits = [0,1,2,3,4,5,6,7,8,9]):\n",
    "    img1s = np.zeros((n, 1, 28, 28))\n",
    "    img2s = np.zeros((n, 1, 28, 28))\n",
    "    target = np.zeros((n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        p = np.random.rand()\n",
    "        d1, d2 = np.random.choice(digits, 2, replace=False)\n",
    "        img1s[i] = data_dict[d1][ int(np.random.rand()*len(data_dict[d1])) ]\n",
    "        \n",
    "        if p > 0.5: # diff\n",
    "            img2s[i] = data_dict[d2][ int(np.random.rand()*len(data_dict[d2])) ]\n",
    "            target[i] = 0\n",
    "        else:       # same\n",
    "            img2s[i] = data_dict[d1][ int(np.random.rand()*len(data_dict[d1])) ]\n",
    "            target[i] = 1     \n",
    "    \n",
    "    return torch.from_numpy(img1s).float(), torch.from_numpy(img2s).float(), torch.from_numpy(target).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "\n",
    "    def calcRep(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        dist = F.cosine_similarity(SiameseNet.calcRep(self, x1), SiameseNet.calcRep(self, x2))  # output 1 for similar image\n",
    "        return dist\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,steps = 1000 ,lr = 0.1, batch_sz = 32, digits = [0,1,2,3,4,5,6,7,8,9]):\n",
    "    model.train()\n",
    "    \n",
    "    for i in range(steps):\n",
    "    \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        img1s, img2s, target = getData(batch_sz, digits)\n",
    "        output = model(img1s, img2s)\n",
    "        \n",
    "        loss = ((output - target)**2).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('step: '+str(i),end='\\t')\n",
    "#             print(loss.item())\n",
    "            test(model, sz = 1000, digits = [0,1,2,3,4,5,6,7,8,9])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, sz = 1000, digits = [0,1,2,3,4,5,6,7,8,9]):\n",
    "    img1s, img2s, target = getData(sz, digits)\n",
    "    output = model(img1s, img2s)\n",
    "    loss = ((output - target)**2).mean()\n",
    "    correct = (output.round()).eq(target.view_as(output)).long().sum().item()    # threshold = .5\n",
    "    total = target.view(-1).size(0)\n",
    "    print('mse = '+str(loss.item())[:10]+'\\t accuracy = '+str(correct)+'/'+str(total)+' = '+str(100.0*correct/total)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiameseNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(SiameseNet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse = 0.49480035\t accuracy = 505/1000 = 50.5\n",
      "mse = 0.15783534\t accuracy = 797/1000 = 79.7\n",
      "mse = 0.12837280\t accuracy = 833/1000 = 83.3\n",
      "mse = 0.13947303\t accuracy = 828/1000 = 82.8\n",
      "mse = 0.15007025\t accuracy = 816/1000 = 81.6\n",
      "mse = 0.13669009\t accuracy = 819/1000 = 81.9\n",
      "mse = 0.15500690\t accuracy = 806/1000 = 80.6\n",
      "mse = 0.15043820\t accuracy = 807/1000 = 80.7\n",
      "mse = 0.15484409\t accuracy = 811/1000 = 81.1\n",
      "mse = 0.14747439\t accuracy = 821/1000 = 82.1\n",
      "mse = 0.14308273\t accuracy = 4124/5000 = 82.48\n",
      "mse = 0.22462043\t accuracy = 3592/5000 = 71.84\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNet()\n",
    "train(model,steps = 1000 ,lr = 0.1, batch_sz = 256, digits = [0,1,5,8,9])\n",
    "\n",
    "test(model, sz = 5000, digits = [0,1,2,3,4,5,6,7,8,9])\n",
    "test(model, sz = 5000, digits = [2,3,4,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
